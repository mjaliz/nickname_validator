{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0c380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load libraries\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from json import loads\n",
    "from cleantext import clean\n",
    "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Dense, Embedding, Dropout, BatchNormalization, Flatten\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, GRU, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.layers import multiply, Input, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam, schedules\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import losses, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "022a1f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleaning(text):\n",
    "    text = text.strip()\n",
    "\n",
    "    # regular cleaning\n",
    "    text = clean(text,\n",
    "      fix_unicode=True,\n",
    "      to_ascii=False,\n",
    "      lower=True,\n",
    "      no_line_breaks=True,\n",
    "      no_urls=True,\n",
    "      no_emails=True,\n",
    "      no_phone_numbers=True,\n",
    "      no_numbers=False,\n",
    "      no_digits=False,\n",
    "      no_currency_symbols=True,\n",
    "      no_punct=False,\n",
    "      replace_with_url=\"\",\n",
    "      replace_with_email=\"\",\n",
    "      replace_with_phone_number=\"\",\n",
    "      replace_with_number=\"\",\n",
    "      replace_with_digit=\"0\",\n",
    "      replace_with_currency_symbol=\"\",\n",
    "    )\n",
    "\n",
    "    # cleaning htmls\n",
    "    text = cleanhtml(text)\n",
    "\n",
    "    # normalizing\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(text)\n",
    "\n",
    "    # removing wierd patterns\n",
    "    wierd_pattern = re.compile(\"[\"\n",
    "      u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "      u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "      u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "      u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "      u\"\\U00002702-\\U000027B0\"\n",
    "      u\"\\U000024C2-\\U0001F251\"\n",
    "      u\"\\U0001f926-\\U0001f937\"\n",
    "      u'\\U00010000-\\U0010ffff'\n",
    "      u\"\\u200d\"\n",
    "      u\"\\u2640-\\u2642\"\n",
    "      u\"\\u2600-\\u2B55\"\n",
    "      u\"\\u23cf\"\n",
    "      u\"\\u23e9\"\n",
    "      u\"\\u231a\"\n",
    "      u\"\\u3030\"\n",
    "      u\"\\ufe0f\"\n",
    "      u\"\\u2069\"\n",
    "      u\"\\u2066\"\n",
    "      # u\"\\u200c\"\n",
    "      u\"\\u2068\"\n",
    "      u\"\\u2067\"\n",
    "      \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    text = wierd_pattern.sub(r'', text)\n",
    "\n",
    "    # removing extra spaces, hashtags\n",
    "    text = re.sub(\"#\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8face56",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'nicknames.csv'\n",
    "train_dir = 'clean_data_balanced.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_dir)\n",
    "df_test = pd.read_csv(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12dd7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.rename(columns={\"nick_name\":\"text\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622d2795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].astype(str)\n",
    "df_train['text'] = df_train['text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029fea9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_train['text'].tolist()\n",
    "labels = df_train['is_offensive'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a677d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.1\n",
    "num_validation_samples = int(validation_split * len(samples))\n",
    "train_samples = samples[:-num_validation_samples]\n",
    "val_samples = samples[-num_validation_samples:]\n",
    "train_labels = labels[:-num_validation_samples]\n",
    "val_labels = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19ef7b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=300)\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(train_samples).batch(64)\n",
    "vectorizer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ecfa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'to', 'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_vocabulary()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9d14a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 2149, 6396,   15,    2,    1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = vectorizer([[\"the cat sat on the mat\"]])\n",
    "output.numpy()[0, :6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4821c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d93f672",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastTextDir = '.'\n",
    "fastText_fa_path = os.path.join(fastTextDir, 'cc.fa.300.vec')\n",
    "fastText_en_path = os.path.join(fastTextDir, 'cc.en.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c52305cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3745941 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "\n",
    "with open(fastText_fa_path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "with open(fastText_en_path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcdbf4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 17028 words (2972 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(voc) + 2\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c35fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4cd0673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 300)         6000600   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, None, 128)         192128    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, None, 128)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, None, 128)        0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, None, 128)         82048     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,373,594\n",
      "Trainable params: 372,994\n",
      "Non-trainable params: 6,000,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded_sequences = embedding_layer(int_sequences_input)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling1D(5)(x)\n",
    "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
    "model = keras.Model(int_sequences_input, preds)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38e9a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n",
    "x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n",
    "\n",
    "y_train = np.array(train_labels)\n",
    "y_val = np.array(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5752881e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72251, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95b861ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72251,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdf69f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1129/1129 [==============================] - 10s 6ms/step - loss: 0.2230 - acc: 0.9113 - val_loss: 0.9394 - val_acc: 0.5764\n",
      "Epoch 2/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.1686 - acc: 0.9342 - val_loss: 0.8799 - val_acc: 0.5769\n",
      "Epoch 3/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.1503 - acc: 0.9416 - val_loss: 0.8855 - val_acc: 0.5754\n",
      "Epoch 4/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.1337 - acc: 0.9487 - val_loss: 0.9789 - val_acc: 0.5773\n",
      "Epoch 5/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.1158 - acc: 0.9560 - val_loss: 1.0050 - val_acc: 0.5782\n",
      "Epoch 6/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0981 - acc: 0.9637 - val_loss: 1.0866 - val_acc: 0.5784\n",
      "Epoch 7/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0815 - acc: 0.9711 - val_loss: 1.0279 - val_acc: 0.5776\n",
      "Epoch 8/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0698 - acc: 0.9756 - val_loss: 1.0104 - val_acc: 0.5803\n",
      "Epoch 9/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0591 - acc: 0.9797 - val_loss: 1.1420 - val_acc: 0.5780\n",
      "Epoch 10/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0510 - acc: 0.9820 - val_loss: 1.1767 - val_acc: 0.5761\n",
      "Epoch 11/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0462 - acc: 0.9852 - val_loss: 1.5630 - val_acc: 0.5764\n",
      "Epoch 12/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0407 - acc: 0.9868 - val_loss: 1.3052 - val_acc: 0.5766\n",
      "Epoch 13/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0379 - acc: 0.9880 - val_loss: 1.1814 - val_acc: 0.5784\n",
      "Epoch 14/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0351 - acc: 0.9888 - val_loss: 1.0538 - val_acc: 0.5783\n",
      "Epoch 15/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0350 - acc: 0.9894 - val_loss: 1.3943 - val_acc: 0.5758\n",
      "Epoch 16/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0296 - acc: 0.9910 - val_loss: 1.4817 - val_acc: 0.5758\n",
      "Epoch 17/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0291 - acc: 0.9915 - val_loss: 1.6870 - val_acc: 0.5797\n",
      "Epoch 18/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0273 - acc: 0.9917 - val_loss: 1.7625 - val_acc: 0.5784\n",
      "Epoch 19/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0256 - acc: 0.9927 - val_loss: 0.8922 - val_acc: 0.5767\n",
      "Epoch 20/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0261 - acc: 0.9926 - val_loss: 0.9784 - val_acc: 0.5773\n",
      "Epoch 21/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0239 - acc: 0.9937 - val_loss: 1.6540 - val_acc: 0.5784\n",
      "Epoch 22/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0266 - acc: 0.9931 - val_loss: 1.7996 - val_acc: 0.5753\n",
      "Epoch 23/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0227 - acc: 0.9941 - val_loss: 1.6875 - val_acc: 0.5769\n",
      "Epoch 24/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0245 - acc: 0.9936 - val_loss: 1.0622 - val_acc: 0.5749\n",
      "Epoch 25/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0239 - acc: 0.9944 - val_loss: 1.9911 - val_acc: 0.5773\n",
      "Epoch 26/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0236 - acc: 0.9945 - val_loss: 1.0882 - val_acc: 0.5756\n",
      "Epoch 27/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0208 - acc: 0.9948 - val_loss: 1.5375 - val_acc: 0.5764\n",
      "Epoch 28/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0223 - acc: 0.9945 - val_loss: 1.0316 - val_acc: 0.5771\n",
      "Epoch 29/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0203 - acc: 0.9953 - val_loss: 1.6009 - val_acc: 0.5777\n",
      "Epoch 30/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0190 - acc: 0.9950 - val_loss: 1.1384 - val_acc: 0.5757\n",
      "Epoch 31/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0177 - acc: 0.9959 - val_loss: 1.1735 - val_acc: 0.5746\n",
      "Epoch 32/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0180 - acc: 0.9953 - val_loss: 1.4045 - val_acc: 0.5761\n",
      "Epoch 33/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0171 - acc: 0.9959 - val_loss: 1.5363 - val_acc: 0.5758\n",
      "Epoch 34/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0179 - acc: 0.9956 - val_loss: 2.2011 - val_acc: 0.5764\n",
      "Epoch 35/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0177 - acc: 0.9962 - val_loss: 1.2107 - val_acc: 0.5769\n",
      "Epoch 36/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0166 - acc: 0.9962 - val_loss: 1.4260 - val_acc: 0.5762\n",
      "Epoch 37/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0158 - acc: 0.9965 - val_loss: 1.5148 - val_acc: 0.5789\n",
      "Epoch 38/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0149 - acc: 0.9963 - val_loss: 1.5704 - val_acc: 0.5763\n",
      "Epoch 39/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0151 - acc: 0.9965 - val_loss: 0.9746 - val_acc: 0.5795\n",
      "Epoch 40/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0150 - acc: 0.9966 - val_loss: 1.9595 - val_acc: 0.5782\n",
      "Epoch 41/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0145 - acc: 0.9966 - val_loss: 1.7041 - val_acc: 0.5762\n",
      "Epoch 42/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0131 - acc: 0.9971 - val_loss: 2.0733 - val_acc: 0.5767\n",
      "Epoch 43/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0143 - acc: 0.9970 - val_loss: 2.4137 - val_acc: 0.5778\n",
      "Epoch 44/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0157 - acc: 0.9965 - val_loss: 1.8174 - val_acc: 0.5776\n",
      "Epoch 45/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0139 - acc: 0.9973 - val_loss: 1.5979 - val_acc: 0.5762\n",
      "Epoch 46/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0157 - acc: 0.9971 - val_loss: 1.6402 - val_acc: 0.5772\n",
      "Epoch 47/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0170 - acc: 0.9967 - val_loss: 1.4821 - val_acc: 0.5772\n",
      "Epoch 48/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0139 - acc: 0.9970 - val_loss: 2.0844 - val_acc: 0.5769\n",
      "Epoch 49/50\n",
      "1129/1129 [==============================] - 6s 6ms/step - loss: 0.0153 - acc: 0.9967 - val_loss: 1.1171 - val_acc: 0.5738\n",
      "Epoch 50/50\n",
      "1129/1129 [==============================] - 7s 6ms/step - loss: 0.0130 - acc: 0.9973 - val_loss: 1.2141 - val_acc: 0.5748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f64e8542250>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    ")\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=50, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eccb221",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_input = keras.Input(shape=(1,), dtype=\"string\")\n",
    "x = vectorizer(string_input)\n",
    "preds = model(x)\n",
    "end_to_end_model = keras.Model(string_input, preds)\n",
    "\n",
    "probabilities = end_to_end_model.predict(\n",
    "    [[\"کیرم دهنت\"]]\n",
    ")\n",
    "\n",
    "np.argmax(probabilities[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce1545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75bff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448b3b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17df325",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train['text'].astype(str)\n",
    "df_test['text'] = df_test['text'].astype(str)\n",
    "\n",
    "df_train['text'] = df_train['text'].apply(cleaning)\n",
    "df_test['text'] = df_test['text'].apply(cleaning)\n",
    "\n",
    "train_data, val_data = train_test_split(df_train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f84459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict(df):\n",
    "    wordDict = {}\n",
    "    for idx, row in enumerate(df.text):\n",
    "        row = re.split(r'([a-zA-Z]+)', row)\n",
    "        row = \" \".join(str(item) for item in row)\n",
    "        words = row.split()\n",
    "        for wrd in words:\n",
    "            if wrd in wordDict:\n",
    "                wordDict[wrd] += 1\n",
    "            else:\n",
    "                wordDict[wrd] = 1\n",
    "    return wordDict\n",
    "\n",
    "train_wordDict = get_dict(df_train)\n",
    "test_wordDict = get_dict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2873e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastTextDir = '.'\n",
    "fastText_fa_path = os.path.join(fastTextDir, 'cc.fa.300.vec')\n",
    "fastText_en_path = os.path.join(fastTextDir, 'cc.en.300.vec')\n",
    "\n",
    "def get_embedding(wordDict):\n",
    "    embeddings_index = {}\n",
    "    with open(fastText_fa_path, encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "            except:\n",
    "                print(\"Warnning\"+str(values)+\" in\" + str(line))\n",
    "            if word in wordDict:\n",
    "                embeddings_index[word] = coefs\n",
    "\n",
    "    with open(fastText_en_path, encoding='utf8') as infile:\n",
    "        for line in infile:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            try:\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "            except:\n",
    "                print(\"Warnning\"+str(values)+\" in\" + str(line))\n",
    "            if word in wordDict:\n",
    "                embeddings_index[word] = coefs\n",
    "    return embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ddc4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = get_embedding(train_wordDict)\n",
    "test_embeddings = get_embedding(test_wordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2920768",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 5500\n",
    "MAX_SEQUENCE_LENGTH = 350\n",
    "\n",
    "content_train = df_train['text']\n",
    "content_test = df_test['text']\n",
    "\n",
    "y_train = np.array(df_train['is_offensive'])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(content_train)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(content_train)\n",
    "test_sequences = tokenizer.texts_to_sequences(content_test)\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a795a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(sentence):\n",
    "    test_sequences = tokenizer.texts_to_sequences([sentence])\n",
    "    test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ab9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embeddings_index = train_embeddings\n",
    "embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980cd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "nClasses = 2\n",
    "\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = BatchNormalization()(embedded_sequences)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "preds = Dense(nClasses, activation='softmax')(x)\n",
    "model = Model(sequence_input, preds)\n",
    "\n",
    "optimizer = Adam(learning_rate=5e-3, beta_1=0.9, beta_2=0.999, \n",
    "                epsilon=1e-07, amsgrad=False)\n",
    "\n",
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                optimizer='rmsprop',\n",
    "                metrics=metrics.SparseCategoricalAccuracy('accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f3f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS =   20 \n",
    "\n",
    "\n",
    "model.fit(train_data, y_train.astype(float),\n",
    "        validation_split=0.2, \n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE)\n",
    "model.save(('nickname.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81594b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

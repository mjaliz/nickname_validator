{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9bd7f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64d28d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('nicknames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6368823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d51750fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 141248 entries, 1 to 141250\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count   Dtype \n",
      "---  ------     --------------   ----- \n",
      " 0   nick_name  141248 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c78092e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = pd.read_csv('profanity_en.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21adc26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>canonical_form_1</th>\n",
       "      <th>canonical_form_2</th>\n",
       "      <th>canonical_form_3</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>severity_rating</th>\n",
       "      <th>severity_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@55</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ssfcker</td>\n",
       "      <td>fuck</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>sexual orientation / gender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@ssfucker</td>\n",
       "      <td>fuck</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>sexual orientation / gender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Severe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@ssfvcker</td>\n",
       "      <td>fuck</td>\n",
       "      <td>ass</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>sexual orientation / gender</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>wnker</td>\n",
       "      <td>wank</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sexual anatomy / sexual acts</td>\n",
       "      <td>other / general insult</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>wop</td>\n",
       "      <td>wop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>racial / ethnic slurs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>wophead</td>\n",
       "      <td>wop</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>racial / ethnic slurs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>zip in the wire</td>\n",
       "      <td>zipperhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>racial / ethnic slurs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>zipperhead</td>\n",
       "      <td>zipperhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>racial / ethnic slurs</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Strong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1598 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 text canonical_form_1 canonical_form_2 canonical_form_3  \\\n",
       "0                  69               69              NaN              NaN   \n",
       "1                 @55              ass              NaN              NaN   \n",
       "2            @ssfcker             fuck              ass              NaN   \n",
       "3           @ssfucker             fuck              ass              NaN   \n",
       "4           @ssfvcker             fuck              ass              NaN   \n",
       "...               ...              ...              ...              ...   \n",
       "1593            wnker             wank              NaN              NaN   \n",
       "1594              wop              wop              NaN              NaN   \n",
       "1595          wophead              wop              NaN              NaN   \n",
       "1596  zip in the wire       zipperhead              NaN              NaN   \n",
       "1597       zipperhead       zipperhead              NaN              NaN   \n",
       "\n",
       "                        category_1                   category_2 category_3  \\\n",
       "0     sexual anatomy / sexual acts                          NaN        NaN   \n",
       "1     sexual anatomy / sexual acts                          NaN        NaN   \n",
       "2     sexual anatomy / sexual acts  sexual orientation / gender        NaN   \n",
       "3     sexual anatomy / sexual acts  sexual orientation / gender        NaN   \n",
       "4     sexual anatomy / sexual acts  sexual orientation / gender        NaN   \n",
       "...                            ...                          ...        ...   \n",
       "1593  sexual anatomy / sexual acts       other / general insult        NaN   \n",
       "1594         racial / ethnic slurs                          NaN        NaN   \n",
       "1595         racial / ethnic slurs                          NaN        NaN   \n",
       "1596         racial / ethnic slurs                          NaN        NaN   \n",
       "1597         racial / ethnic slurs                          NaN        NaN   \n",
       "\n",
       "      severity_rating severity_description  \n",
       "0                 1.0                 Mild  \n",
       "1                 1.0                 Mild  \n",
       "2                 2.8               Severe  \n",
       "3                 2.8               Severe  \n",
       "4                 2.4               Strong  \n",
       "...               ...                  ...  \n",
       "1593              1.0                 Mild  \n",
       "1594              2.4               Strong  \n",
       "1595              2.2               Strong  \n",
       "1596              1.4                 Mild  \n",
       "1597              2.4               Strong  \n",
       "\n",
       "[1598 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bad86bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "profanity = df_p['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fc5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nicknames = df['nick_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f53dc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_en = pd.read_csv('bad-words.csv')\n",
    "\n",
    "df_sw_en = df_sw_en.rename(columns={\"jigaboo\":\"word\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf7b7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw = pd.read_json(\"data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4afe78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_concat = pd.concat([df_sw_en, df_sw, pd.DataFrame({'word':profanity})], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "661df90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mound of venus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asslover</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s&amp;m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>queaf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whitetrash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>wnker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>wop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3512</th>\n",
       "      <td>wophead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>zip in the wire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>zipperhead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3515 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word\n",
       "0      mound of venus\n",
       "1            asslover\n",
       "2                 s&m\n",
       "3               queaf\n",
       "4          whitetrash\n",
       "...               ...\n",
       "3510            wnker\n",
       "3511              wop\n",
       "3512          wophead\n",
       "3513  zip in the wire\n",
       "3514       zipperhead\n",
       "\n",
       "[3515 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sw_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f68b3abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_concat.to_csv('swear_words.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2b6c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_concat['is_offensive'] = 1\n",
    "df_sw_concat.rename(columns={'word':'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c047c95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sw_concat.to_csv('swear_words.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fea654fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_sw_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8aeb2741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3515, 65)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "features = tfidf.fit_transform(data.word).toarray()\n",
    "labels = data.is_offensive\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b3b23051",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['word'], data['is_offensive'], random_state = 0)\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c2e71d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(count_vect.transform([\"hello\"]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "096c493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(word):\n",
    "    return clf.predict(count_vect.transform([word]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa6074d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_offensive'] = df['nick_name'].apply(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5030489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nick_name</th>\n",
       "      <th>is_offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nick_name, is_offensive]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['is_offensive']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafc77e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af6e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc20ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "081598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "swear_words = df_sw_concat['word'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40584866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/distiluse-base-multilingual-cased-v2')\n",
    "\n",
    "sentences = swear_words\n",
    "embeddings = model.encode(sentences)\n",
    "embeds = []\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    embeds.append(embedding)\n",
    "    \n",
    "df_sw_concat['embed'] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "831aa300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mound of venus</td>\n",
       "      <td>[-0.0012805157, 0.00071827986, -0.003063048, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asslover</td>\n",
       "      <td>[0.013014284, 0.00821337, -0.030748818, -0.029...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>s&amp;m</td>\n",
       "      <td>[0.039454542, 0.01372775, -0.00965603, 0.00609...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>queaf</td>\n",
       "      <td>[0.027138958, 0.025423177, -0.03308883, -0.017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>whitetrash</td>\n",
       "      <td>[-0.0055718645, 0.037569765, -0.027341058, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>wnker</td>\n",
       "      <td>[0.008764469, 0.0034810754, -0.018183053, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3511</th>\n",
       "      <td>wop</td>\n",
       "      <td>[0.043651517, 0.022549834, -0.021462087, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3512</th>\n",
       "      <td>wophead</td>\n",
       "      <td>[0.023418043, 0.024824606, -0.0039961813, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3513</th>\n",
       "      <td>zip in the wire</td>\n",
       "      <td>[-0.018007044, -0.011607245, -0.07618794, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>zipperhead</td>\n",
       "      <td>[0.018734444, 0.00873077, -0.042612303, -0.031...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3515 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 word                                              embed\n",
       "0      mound of venus  [-0.0012805157, 0.00071827986, -0.003063048, 0...\n",
       "1            asslover  [0.013014284, 0.00821337, -0.030748818, -0.029...\n",
       "2                 s&m  [0.039454542, 0.01372775, -0.00965603, 0.00609...\n",
       "3               queaf  [0.027138958, 0.025423177, -0.03308883, -0.017...\n",
       "4          whitetrash  [-0.0055718645, 0.037569765, -0.027341058, -0....\n",
       "...               ...                                                ...\n",
       "3510            wnker  [0.008764469, 0.0034810754, -0.018183053, -0.0...\n",
       "3511              wop  [0.043651517, 0.022549834, -0.021462087, -0.05...\n",
       "3512          wophead  [0.023418043, 0.024824606, -0.0039961813, -0.0...\n",
       "3513  zip in the wire  [-0.018007044, -0.011607245, -0.07618794, 0.00...\n",
       "3514       zipperhead  [0.018734444, 0.00873077, -0.042612303, -0.031...\n",
       "\n",
       "[3515 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sw_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21bf771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nicknames\n",
    "embeddings = model.encode(sentences)\n",
    "embeds = []\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    embeds.append(embedding)\n",
    "    \n",
    "df['embed'] = embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23c28908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nick_name</th>\n",
       "      <th>embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>[0.011652807, 0.009943119, -0.02741476, -0.035...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌?</td>\n",
       "      <td>[0.02485765, -0.0080936635, -0.04724872, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>‌ ‌ ‌- emilia&lt;3</td>\n",
       "      <td>[0.033218604, -0.011780523, -0.035756074, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>⁭⁫⁪⁫⁬⁭⁫⁪⁫⁬ ⁭hop_pa</td>\n",
       "      <td>[0.050225995, 0.03244206, -0.016792454, 0.0230...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>‌ (Z)</td>\n",
       "      <td>[0.0041125217, 0.05452588, -0.05655993, -0.016...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141246</th>\n",
       "      <td>𝓜𝓸𝓱𝓪𝓶𝓶𝓪𝓭 𝓡𝓮𝔃𝓪</td>\n",
       "      <td>[0.009509911, -0.008516736, -0.015367321, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141247</th>\n",
       "      <td>🤍🖤🤎💜💙💚💛🧡⁦❤️⁩</td>\n",
       "      <td>[0.01359888, -0.003466455, -0.027659642, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141248</th>\n",
       "      <td>𝐌𝐚𝐫𝐳𝐢𝐲𝐞𝐡🌱</td>\n",
       "      <td>[0.01359888, -0.003466455, -0.027659642, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141249</th>\n",
       "      <td>𝓐𝓶𝓲𝓻𝓱𝓸𝓼𝓮𝓲𝓷</td>\n",
       "      <td>[0.01359888, -0.003466455, -0.027659642, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141250</th>\n",
       "      <td>𝒎𝒂𝒉𝒅𝒐𝒌𝒉𝒕𝒂𝒓𝒚</td>\n",
       "      <td>[0.013598873, -0.0034664597, -0.027659642, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141251 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  nick_name                                              embed\n",
       "0                       NaN  [0.011652807, 0.009943119, -0.02741476, -0.035...\n",
       "1          ‌ ‌ ‌ ‌ ‌ ‌ ‌ ‌?  [0.02485765, -0.0080936635, -0.04724872, -0.07...\n",
       "2           ‌ ‌ ‌- emilia<3  [0.033218604, -0.011780523, -0.035756074, 0.00...\n",
       "3        ⁭⁫⁪⁫⁬⁭⁫⁪⁫⁬ ⁭hop_pa  [0.050225995, 0.03244206, -0.016792454, 0.0230...\n",
       "4                     ‌ (Z)  [0.0041125217, 0.05452588, -0.05655993, -0.016...\n",
       "...                     ...                                                ...\n",
       "141246        𝓜𝓸𝓱𝓪𝓶𝓶𝓪𝓭 𝓡𝓮𝔃𝓪  [0.009509911, -0.008516736, -0.015367321, -0.0...\n",
       "141247         🤍🖤🤎💜💙💚💛🧡⁦❤️⁩  [0.01359888, -0.003466455, -0.027659642, -0.03...\n",
       "141248            𝐌𝐚𝐫𝐳𝐢𝐲𝐞𝐡🌱  [0.01359888, -0.003466455, -0.027659642, -0.03...\n",
       "141249           𝓐𝓶𝓲𝓻𝓱𝓸𝓼𝓮𝓲𝓷  [0.01359888, -0.003466455, -0.027659642, -0.03...\n",
       "141250          𝒎𝒂𝒉𝒅𝒐𝒌𝒉𝒕𝒂𝒓𝒚  [0.013598873, -0.0034664597, -0.027659642, -0....\n",
       "\n",
       "[141251 rows x 2 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b9bb1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "swears = []\n",
    "for i in range(len(df)):\n",
    "    score = cosine_similarity(df_sw_concat['embed'].tolist(), df['embed'].iloc[i].reshape(1, -1))\n",
    "    argmax = np.argmax(score)\n",
    "    if score[argmax][0] > 0.98:\n",
    "        swears.append(df['nick_name'].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09d88719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swears)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cad284aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "a_s_h_sh\n",
      "a_s_r_a\n",
      "abo\n",
      "allah\n",
      "amateur\n",
      "American\n",
      "anal\n",
      "arab\n",
      "areola\n",
      "ass\n",
      "asss\n",
      "babe\n",
      "barf\n",
      "bbc\n",
      "bibi\n",
      "black\n",
      "caca\n",
      "canadian\n",
      "cool\n",
      "crash\n",
      "cristian\n",
      "Death\n",
      "Demon\n",
      "desire\n",
      "devil\n",
      "devill\n",
      "dick\n",
      "die\n",
      "doodoo\n",
      "Dumb\n",
      "ehsj\n",
      "ero\n",
      "fack\n",
      "faith\n",
      "fat\n",
      "fatah\n",
      "fatahne\n",
      "fear\n",
      "fhg\n",
      "fight\n",
      "fire\n",
      "fuck\n",
      "fuck you\n",
      "fucker\n",
      "gangbang\n",
      "gay\n",
      "gin\n",
      "god\n",
      "goddamnrad\n",
      "gun\n",
      "guts\n",
      "hab\n",
      "hajj\n",
      "hama\n",
      "hamse\n",
      "harem\n",
      "hell\n",
      "hiv\n",
      "ho\n",
      "homo\n",
      "idiot\n",
      "Italiano\n",
      "jaf\n",
      "japoni\n",
      "jiso\n",
      "juya\n",
      "kid\n",
      "kill\n",
      "kin\n",
      "kink\n",
      "kinko\n",
      "kkk\n",
      "kkkk\n",
      "klan\n",
      "knife\n",
      "Latin\n",
      "lolita\n",
      "loser\n",
      "lucifer\n",
      "mafia\n",
      "mama\n",
      "mamaj\n",
      "mamas\n",
      "mami\n",
      "mamia\n",
      "mamina\n",
      "mammadi\n",
      "mamo\n",
      "mams\n",
      "mfh\n",
      "mfk\n",
      "milf\n",
      "moslem\n",
      "murder\n",
      "muslim\n",
      "muslimgh\n",
      "negah\n",
      "ney\n",
      "nia\n",
      "niaish\n",
      "nicka\n",
      "nono\n",
      "noon\n",
      "osama\n",
      "panty\n",
      "pee\n",
      "peter\n",
      "po\n",
      "pom\n",
      "poo\n",
      "poon\n",
      "pooo\n",
      "poya\n",
      "randy\n",
      "roach\n",
      "russian\n",
      "s_a\n",
      "s_u_d_eh\n",
      "s.b.h\n",
      "s&m\n",
      "sambo\n",
      "satan\n",
      "sex\n",
      "shit\n",
      "sima\n",
      "slut\n",
      "sniper\n",
      "sob\n",
      "sos\n",
      "swallow\n",
      "tit\n",
      "uk\n",
      "usama\n",
      "vfm\n",
      "wahaj\n",
      "wf\n",
      "Whiskey\n",
      "willie\n",
      "willy\n",
      "wtf\n",
      "xx\n",
      "xxx\n",
      "xxxx\n",
      "ya\n",
      "yaa\n",
      "zo\n",
      "zoa\n",
      "اسب\n",
      "انگل\n",
      "انی\n",
      "بدبخت\n",
      "پدرت\n",
      "پفیوز\n",
      "ترک\n",
      "جنده\n",
      "خر\n",
      "خری\n",
      "خوب\n",
      "زارت\n",
      "سولاخ\n",
      "سیاه\n",
      "شوتی\n",
      "عرب\n",
      "عربي\n",
      "عربية\n",
      "عمه ننه\n",
      "عن\n",
      "فارس\n",
      "کثافت\n",
      "کرم\n",
      "کس\n",
      "کس‌کش\n",
      "کص\n",
      "کص خل\n",
      "کون\n",
      "کونی\n",
      "کیر\n",
      "گاو\n",
      "گوز\n",
      "گوه\n",
      "لاشی\n",
      "لر\n",
      "ملنگ\n",
      "نگایدم\n",
      "نگاییدم\n",
      "نوب\n",
      "هیزم\n"
     ]
    }
   ],
   "source": [
    "for s in swears:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c96a6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_swear(word):\n",
    "    word = str(word)\n",
    "    for i in range(len(df_sw_concat)):\n",
    "        swear = str(df_sw_concat.iloc[i]['word'])\n",
    "        if word in swear or swear in word:\n",
    "            return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fc21948d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_swear\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnick_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_swear\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[43], line 4\u001b[0m, in \u001b[0;36mcheck_swear\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      2\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(word)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df_sw_concat)):\n\u001b[0;32m----> 4\u001b[0m     swear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mdf_sw_concat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mword\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m swear \u001b[38;5;129;01mor\u001b[39;00m swear \u001b[38;5;129;01min\u001b[39;00m word:\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1073\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1070\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1072\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[0;32m-> 1073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/indexing.py:1627\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1624\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m   1625\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_integer(key, axis)\n\u001b[0;32m-> 1627\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/frame.py:3716\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3714\u001b[0m \u001b[38;5;66;03m# irow\u001b[39;00m\n\u001b[1;32m   3715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 3716\u001b[0m     new_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfast_xs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3718\u001b[0m     \u001b[38;5;66;03m# if we are a copy, mark as such\u001b[39;00m\n\u001b[1;32m   3719\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(new_mgr\u001b[38;5;241m.\u001b[39marray, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m new_mgr\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1126\u001b[0m, in \u001b[0;36mBlockManager.fast_xs\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m cast(ExtensionDtype, dtype)\n\u001b[1;32m   1124\u001b[0m     result \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mconstruct_array_type()\u001b[38;5;241m.\u001b[39m_from_sequence(result, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m-> 1126\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SingleBlockManager(block, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/LEARNit/nickname-validator/venv/lib/python3.10/site-packages/pandas/core/internals/blocks.py:2183\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim)\u001b[0m\n\u001b[1;32m   2180\u001b[0m klass \u001b[38;5;241m=\u001b[39m get_block_type(values\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   2182\u001b[0m values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(values)\n\u001b[0;32m-> 2183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplacement\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df['is_swear'] = df['nick_name'].apply(check_swear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf2669b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b53b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'swear':swears})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eabcf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = cosine_similarity(df['embed'].tolist(), df_sw['embed'].iloc[4].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax = np.argmax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdeaf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "score[argmax][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eaedad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[argmax]['nick_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c50023c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sex' in 'hamedsexy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a251140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
